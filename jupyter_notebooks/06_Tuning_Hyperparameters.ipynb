{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Tuning Hyperparameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* We will tune the hyperparameters of for Logistic Regression and an Adaptive Boost model.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Training and Testing data sets from notebook 04.\n",
        "* Insights developed in the previous notebook.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* We will have saved models with tuned hyper parameters at the end of this notebook.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* We are making some philosophical assumptions about the nature of hyperparameters. The basic assumption is that the performance of a model trained with hyperparameters that are \"near enough\" to each other will perform \"similarly enough.\" This is the idea that the performance of the model depends _continuously_ on the hyperparameters. We in fact assume a certain amount of regularity of this dependence. In partial differential equations (pdes), the kind of behavior we are assuming is characteristic of elliptic pdes. We do not have a technical reason for believing this. We assume this is an active area of research for various models, but in general it is outside the scope of this project. It does influence our decision in how we go about searching for good hyperparameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory\n",
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/pp5-ml-dashboard\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "home_dir = '/workspace/pp5-ml-dashboard'\n",
        "os.chdir(home_dir)\n",
        "current_dir = os.getcwd()\n",
        "print(current_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now load our training and test sets, as well as some of the packages that we will be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from src.utils import get_df, save_df\n",
        "\n",
        "train_dir = 'datasets/train/csv'\n",
        "X_TrainSet = get_df('X_TrainSet',train_dir)\n",
        "Y_TrainSet = get_df('Y_TrainSet',train_dir)\n",
        "\n",
        "test_dir = 'datasets/test/csv'\n",
        "X_TestSet = get_df('X_TestSet',test_dir)\n",
        "Y_TestSet = get_df('Y_TestSet',test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Pipeline and Grid Search set up\n",
        "We recall the code for building our pipelines and the grid search that we performed in the last notebook. Note that some of the constants have changed. \n",
        "\n",
        "We have modified the pipeline to see how feateure selection impacts the performance. Note that setting `thresh=1` essentially removes the `'corr_selector'` step from the pipeline. We will eventually remove this step from the pipeline once we have selected a value for `thresh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.selection import DropFeatures, SmartCorrelatedSelection\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Constants needed for feature engineering\n",
        "TO_DROP = ['ftm_away', 'plus_minus_home', 'fg3m_away', 'pts_away', 'play_off',\n",
        "           'fgm_away', 'pts_home', 'fg3m_home', 'ftm_home', 'fgm_home',\n",
        "           'season']\n",
        "THRESH = 0.6\n",
        "TRANSFORMS = {'box_cox':(vt.BoxCoxTransformer,False),\n",
        "              'yeo_johnson':(vt.YeoJohnsonTransformer,False)}\n",
        "TRANSFORM_ASSIGNMENTS = {\n",
        "    'yeo_johnson': ['dreb_away', 'blk_home', 'oreb_away', 'fta_away',\n",
        "                    'dreb_home', 'ast_home', 'stl_away', 'stl_home',\n",
        "                    'reb_away', 'oreb_home', 'pf_away', 'pf_home'],\n",
        "    'box_cox': ['ast_away', 'fta_home']\n",
        "                            }\n",
        "\n",
        "\n",
        "def base_pipeline(thresh=THRESH):\n",
        "    pipeline = Pipeline([\n",
        "        ('dropper', DropFeatures(features_to_drop=TO_DROP)),\n",
        "        ('corr_selector', SmartCorrelatedSelection(method=\"pearson\",\n",
        "                                                   threshold=thresh,\n",
        "                                                   selection_method=\"variance\")\n",
        "                                                   )\n",
        "                        ])\n",
        "    return pipeline\n",
        "\n",
        "    \n",
        "def add_transformations(pipeline, transform_assignments):\n",
        "    # This needs to be called after the above is fit so that the correlation selector has that attr\n",
        "    dropping = pipeline['corr_selector'].features_to_drop_\n",
        "    \n",
        "    new_assignments = { key: [val for val in value if val not in dropping] \n",
        "                       for key,value in transform_assignments.items()}\n",
        "    for transform, targets in new_assignments.items():\n",
        "        if not targets:\n",
        "            continue\n",
        "        pipeline.steps.append(\n",
        "            (transform, TRANSFORMS[transform][0](variables=targets))\n",
        "            )\n",
        "    pipeline.steps.append(('scaler', StandardScaler()))\n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "\n",
        "MODELS = {\n",
        "    'LogisticRegression': LogisticRegression,\n",
        "    'AdaBoost': AdaBoostClassifier,\n",
        "}\n",
        "\n",
        "def create_pipe(model_name, random_state=42, params={}):\n",
        "    model = MODELS[model_name](random_state=random_state,**params)\n",
        "    base_pipe = base_pipeline()\n",
        "    base_pipe.fit(X_TrainSet)\n",
        "    pipe= add_transformations(base_pipe,TRANSFORM_ASSIGNMENTS)\n",
        "    pipe.steps.append((\"feat_selection\", SelectFromModel(model)))\n",
        "    pipe.steps.append(('model',model))\n",
        "    pipe.name = model_name\n",
        "    return pipe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we have the code for our grid search. As we will be treating `thresh` as a hyperparameter, it will be slightly different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# to suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import logging\n",
        "logging.captureWarnings(True)\n",
        "os.environ['PYTHONWARNINGS']='ignore'\n",
        "\n",
        "\n",
        "def grid_search(X_train, y_train,pipe,param_grid={},verbosity=1):\n",
        "    print(f\"### Beginning grid search for {pipe.name} ###\") \n",
        "    grid=GridSearchCV(estimator=pipe,\n",
        "                    param_grid=param_grid,\n",
        "                    cv=5,\n",
        "                    n_jobs=-2,\n",
        "                    verbose=verbosity,\n",
        "                    scoring=['accuracy','precision'],\n",
        "                    refit='precision')\n",
        "    grid.fit(X_train,y_train)\n",
        "    return grid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Logistic Regression\n",
        "Logistic Regression models have many hyperparameters. We will focus on:\n",
        "* `penalty` is a regularization parameter.\n",
        "* `solver` specifies the type of algorithm used.\n",
        "* `C` controls the strength of the penalty.\n",
        "\n",
        "Not all penalties work for each solver.\n",
        "\n",
        "These will be our initial choice for hyperparameters. They will help us narrow down our search and find other ranges to test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "thresholds = [round(0.1*i,2) for i in range(5,11)]\n",
        "C = [10**(2*i+1) for i in range(-2,2)]\n",
        "solver = ['newton-cg', 'newton-cholesky', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "penalty = ['none', 'l1', 'l2', 'elasticnet']\n",
        "param_grid = [\n",
        "            {'C':C,\n",
        "             'solver':['lbfgs','newton-cg','newton-cholesky','sag'],\n",
        "             'penalty':['l2',None]},\n",
        "            {'C':C,\n",
        "             'solver':['liblinear'],\n",
        "             'penalty':['l1','l2']},\n",
        "            {'C':C,\n",
        "             'solver':['saga'],\n",
        "             'penalty':['l1','l2',None,'elasticnet']}\n",
        "              ]\n",
        "logistic_param_grid = [{'model__'+key:value \n",
        "                                for key,value in param_dict.items()}\n",
        "                                for param_dict in param_grid]\n",
        "for params in logistic_param_grid:\n",
        "    params['corr_selector__threshold']=thresholds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are ready to do the grid search. We expect this to go well since Logistic Regression was our best performing model without any tuning. This initial training will help us establish a range to further tune the hyperparamters in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils import save_df, get_df\n",
        "\n",
        "\n",
        "def get_grid_results_df(pipe, name, dir, param_grid={}, verbosity=2):\n",
        "    try:\n",
        "        results_df = get_df(name, dir)\n",
        "    except FileNotFoundError:\n",
        "        pipe_grid_search = grid_search(\n",
        "            X_TrainSet, Y_TrainSet, pipe, param_grid=param_grid, verbosity=verbosity\n",
        "        )\n",
        "        results_df = pd.DataFrame(pipe_grid_search.cv_results_)\n",
        "        save_df(results_df, name, dir)\n",
        "        # this normalizes the types\n",
        "        results_df = get_df(name, dir)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "logistic_pipe = create_pipe(\"LogisticRegression\")\n",
        "results_name = \"logistic_grid_results_v1\"\n",
        "dir = \"experiment_results/tuning/grids\"\n",
        "\n",
        "logistic_results_df_v1 = get_grid_results_df(\n",
        "    logistic_pipe, results_name, dir, param_grid=logistic_param_grid\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let's look at what the best choices are at this stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for current model:\n",
            "threshold: 0.8\n",
            "C: 0.001\n",
            "penalty: None\n",
            "solver: lbfgs\n",
            "Avg. Precision: 88.70480236491775%.\n",
            "Avg. Accuracy: 87.18030112459309%.\n",
            "\n",
            "threshold: 0.8\n",
            "C: 0.001\n",
            "penalty: None\n",
            "solver: newton-cg\n",
            "Avg. Precision: 88.70480236491775%.\n",
            "Avg. Accuracy: 87.18030112459309%.\n",
            "\n",
            "threshold: 0.8\n",
            "C: 0.001\n",
            "penalty: None\n",
            "solver: newton-cholesky\n",
            "Avg. Precision: 88.70480236491775%.\n",
            "Avg. Accuracy: 87.18030112459309%.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from src.model_eval import get_best_params_df\n",
        "\n",
        "get_best_params_df(logistic_results_df_v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly, the best correlation threshold is 0.8. The models have the same scores. Lets collect all of the parameters that have the same scores. We order the scores not by count but by score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---  Score Counts  ---\n",
            "Precision: 0.8870480236491776, Accuracy: 0.8718030112459308\n",
            "Count: 34\n",
            "\n",
            "Precision: 0.887042467962182, Accuracy: 0.8717743374108053\n",
            "Count: 2\n",
            "\n",
            "Precision: 0.8870152603635608, Accuracy: 0.8718030194666403\n",
            "Count: 1\n",
            "\n",
            "Precision: 0.8869924718004393, Accuracy: 0.8718890245305975\n",
            "Count: 1\n",
            "\n",
            "---  Score Stats  ---\n",
            "Most Common: Precision: -1\n",
            "             Accuracy: -1\n",
            "             Count: 76\n",
            "Max Score: Precision: 0.8870480236491776\n",
            "           Accuracy: 0.8718030112459308\n",
            "           Count: 34\n",
            "Max Precision: 0.8870480236491776\n",
            "Max Accuracy: 0.871917694255368\n"
          ]
        }
      ],
      "source": [
        "from src.model_eval import present_score_counts, score_stats\n",
        "\n",
        "\n",
        "present_score_counts(logistic_results_df_v1)\n",
        "best_score = score_stats(logistic_results_df_v1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most common scores are `nan` (we changed the score to be -1 if a `nan` value was showing up). This happens when a choice of parameters does not work well together. We won't worry about this as we are able to get quite good precision and accuracy with this first pass. Note that the best accuracy score is not far from the accuracy of the model with the best precision.\n",
        "\n",
        "34 different choices of parameters had the best performance. We would like to see what these estimators had in common and look at neighborhoods around these parameters to see if we can improve the performance before moving on to the next model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model__solver: saga, Count: 8\n",
            "model__solver: lbfgs, Count: 6\n",
            "model__solver: newton-cg, Count: 6\n",
            "model__solver: newton-cholesky, Count: 6\n",
            "model__solver: sag, Count: 5\n",
            "model__solver: liblinear, Count: 3\n",
            "model__penalty: None, Count: 20\n",
            "model__penalty: l2, Count: 10\n",
            "model__penalty: l1, Count: 4\n",
            "model__C: 1000, Count: 13\n",
            "model__C: 10, Count: 11\n",
            "model__C: 0.001, Count: 5\n",
            "model__C: 0.1, Count: 5\n",
            "corr_selector__threshold: 0.8, Count: 34\n"
          ]
        }
      ],
      "source": [
        "from src.model_eval import present_param_counts\n",
        "\n",
        "present_param_counts(logistic_results_df_v1, best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will make the following modifications to `logistic_param_grid`:\n",
        "* remove `'liblinear'` and `'sag'` as they were used the least,\n",
        "* pick a neighborhood around 0.8 for correlation threshold,\n",
        "* focus on penalties `'l2'` and `'None'`\n",
        "* focus on the range 1 to 1000 for `C`\n",
        "\n",
        "We will see if focusing gives us any improvement in score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils import divide_range\n",
        "\n",
        "thresholds = divide_range(0.75,0.85,5)\n",
        "C = divide_range(1,1000,6)\n",
        "logistic_param_grid_v2=[{'model__C': C,\n",
        "  'model__solver': ['lbfgs', 'newton-cg', 'newton-cholesky','saga'],\n",
        "  'model__penalty': ['l2', None],\n",
        "  'corr_selector__threshold': thresholds}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "logistic_results_df_v2 = get_grid_results_df(logistic_pipe,\n",
        "                                             'logistic_results_df_v2', dir, \n",
        "                                             param_grid=logistic_param_grid_v2,\n",
        "                                             verbosity=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's proceed by doing the analysis we did above of the results of this grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for current model:\n",
            "threshold: 0.77\n",
            "C: 1.0\n",
            "penalty: None\n",
            "solver: lbfgs\n",
            "Avg. Precision: 88.70480236491775%.\n",
            "Avg. Accuracy: 87.18030112459309%.\n",
            "\n",
            "threshold: 0.77\n",
            "C: 1.0\n",
            "penalty: None\n",
            "solver: newton-cg\n",
            "Avg. Precision: 88.70480236491775%.\n",
            "Avg. Accuracy: 87.18030112459309%.\n",
            "\n",
            "threshold: 0.77\n",
            "C: 1.0\n",
            "penalty: None\n",
            "solver: newton-cholesky\n",
            "Avg. Precision: 88.70480236491775%.\n",
            "Avg. Accuracy: 87.18030112459309%.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "get_best_params_df(logistic_results_df_v2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, very similar scores. So let's analyze the scores that showed up as we did before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---  Score Counts  ---\n",
            "Precision: 0.8870480236491776, Accuracy: 0.8718030112459308\n",
            "Count: 156\n",
            "\n",
            "Precision: 0.8868889708447109, Accuracy: 0.871716997961264\n",
            "Count: 3\n",
            "\n",
            "Precision: 0.8868481163465816, Accuracy: 0.8716883282364934\n",
            "Count: 9\n",
            "\n",
            "Precision: 0.8753149006289227, Accuracy: 0.8584414808786294\n",
            "Count: 78\n",
            "\n",
            "---  Score Stats  ---\n",
            "Most Common: Precision: 0.8870480236491776\n",
            "             Accuracy: 0.8718030112459308\n",
            "             Count: 156\n",
            "Max Score: Precision: 0.8870480236491776\n",
            "           Accuracy: 0.8718030112459308\n",
            "           Count: 156\n",
            "Max Precision: 0.8870480236491776\n",
            "Max Accuracy: 0.8718030112459308\n"
          ]
        }
      ],
      "source": [
        "present_score_counts(logistic_results_df_v2)\n",
        "best_score = score_stats(logistic_results_df_v2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our most common score is our best score. It seems like we have chosen a good range of parameters since many of the combinations yield good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model__solver: lbfgs, Count: 39\n",
            "model__solver: newton-cg, Count: 39\n",
            "model__solver: newton-cholesky, Count: 39\n",
            "model__solver: saga, Count: 39\n",
            "model__penalty: None, Count: 84\n",
            "model__penalty: l2, Count: 72\n",
            "model__C: 167.5, Count: 24\n",
            "model__C: 334.0, Count: 24\n",
            "model__C: 500.5, Count: 24\n",
            "model__C: 667.0, Count: 24\n",
            "model__C: 833.5, Count: 24\n",
            "model__C: 1000.0, Count: 24\n",
            "model__C: 1.0, Count: 12\n",
            "corr_selector__threshold: 0.77, Count: 52\n",
            "corr_selector__threshold: 0.79, Count: 52\n",
            "corr_selector__threshold: 0.8099999999999999, Count: 52\n"
          ]
        }
      ],
      "source": [
        "present_param_counts(logistic_results_df_v2, best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All of the choices of parameters seem to be performing equally well, with insignificant exceptions. We will have to look at other metrics to determine distinguish between these choices of parameters. Things such as training time statistics, and standard deviation of the scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "std_test_precision: 0.0054844417109899\n",
            "std_test_accuracy: 0.004118208175772\n",
            "count: 156\n"
          ]
        }
      ],
      "source": [
        "best_results = logistic_results_df_v2.query(f'mean_test_precision == {best_score[0]} and mean_test_accuracy == {best_score[1]}')\n",
        "std_score_counts = {}\n",
        "\n",
        "for _, row in best_results.iterrows():\n",
        "    std_score = (row['std_test_precision'], row['std_test_accuracy'])\n",
        "    if std_score in std_score_counts:\n",
        "        std_score_counts[std_score] += 1\n",
        "    else:\n",
        "        std_score_counts[std_score] = 1\n",
        "for key, value in std_score_counts.items():\n",
        "    print(f\"std_test_precision: {key[0]}\"\n",
        "          f\"\\nstd_test_accuracy: {key[1]}\"\n",
        "          f\"\\ncount: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So standard deviation will not help us distinguish either. This is annoying, but good. We will see how the models perform on the test data set.\n",
        "\n",
        "Note: If you are tinkering and running cells multiple times, we recommend commenting out the code in the following three cells. They take a bit even after they have already been run the first time since we aren't saving the large number of pipelines and models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nbest_pipes = []\\nfor param_dict in best_params:\\n    base_pipe = create_pipe(\\'LogisticRegression\\')\\n    pipe = base_pipe.set_params(**param_dict)\\n    pipe.param_dict = param_dict\\n    best_pipes.append(pipe)\\n\\nmodel_params = {key.split(\\'__\\')[0]:key.split(\\'__\\')[1]\\n                for key in best_params[0].keys()}\\ncount = 0\\nfor pipe in best_pipes:\\n    print(f\"Pipe {count}:\")\\n    for step in pipe.get_params()[\\'steps\\']:\\n        if step[0] in model_params:\\n            param = model_params[step[0]]\\n            value = step[1].get_params()[param]\\n            print(f\"{step[0]}\"\\n                  f\"\\n{param}: {value}\")\\n    print()\\n    count += 1\\n    if count>=2:\\n        break\\n'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "def parameter_dicts(results_df, best_score):\n",
        "    relevant = results_df.query(f'mean_test_precision == {best_score[0]} and mean_test_accuracy == {best_score[1]}')\n",
        "    param_dicts = [ast.literal_eval(param_dict) for param_dict in relevant['params'].values]\n",
        "    return param_dicts\n",
        "\n",
        "best_params = parameter_dicts(logistic_results_df_v2, best_score)\n",
        "'''\n",
        "best_pipes = []\n",
        "for param_dict in best_params:\n",
        "    base_pipe = create_pipe('LogisticRegression')\n",
        "    pipe = base_pipe.set_params(**param_dict)\n",
        "    pipe.param_dict = param_dict\n",
        "    best_pipes.append(pipe)\n",
        "\n",
        "model_params = {key.split('__')[0]:key.split('__')[1]\n",
        "                for key in best_params[0].keys()}\n",
        "count = 0\n",
        "for pipe in best_pipes:\n",
        "    print(f\"Pipe {count}:\")\n",
        "    for step in pipe.get_params()['steps']:\n",
        "        if step[0] in model_params:\n",
        "            param = model_params[step[0]]\n",
        "            value = step[1].get_params()[param]\n",
        "            print(f\"{step[0]}\"\n",
        "                  f\"\\n{param}: {value}\")\n",
        "    print()\n",
        "    count += 1\n",
        "    if count>=2:\n",
        "        break\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now train all of the above pipelines and evaluate them on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'count = 0\\nfor pipe in best_pipes:\\n    print(f\"Training pipe {count}:\")\\n    print(pipe.param_dict)\\n    pipe.fit(X_TrainSet, Y_TrainSet)\\n    count+=1\\n    '"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''count = 0\n",
        "for pipe in best_pipes:\n",
        "    print(f\"Training pipe {count}:\")\n",
        "    print(pipe.param_dict)\n",
        "    pipe.fit(X_TrainSet, Y_TrainSet)\n",
        "    count+=1\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndef evaluate_param_on_test_set(pipe,X_test, Y_test):\\n    y_pred = pipe.predict(X_test)\\n    accuracy = accuracy_score(Y_test, y_pred)\\n    precision = precision_score(Y_test, y_pred)\\n    recall = recall_score(Y_test, y_pred)\\n    f1 = f1_score(Y_test, y_pred)\\n    return ((precision, accuracy, recall, f1), pipe.param_dict)\\n\\n\\ndef evaluate_and_sort(fitted_pipes, X_test, Y_test):\\n    evaluations = [evaluate_param_on_test_set(pipe, X_test, Y_test)\\n               for pipe in fitted_pipes]\\n    evaluation_dict = {}\\n    for eval in evaluations:\\n        if eval[0] in evaluation_dict:\\n            evaluation_dict[eval[0]].append(eval[1])\\n        else:\\n            evaluation_dict[eval[0]] = [eval[1]]\\n    sorted_eval_dict = {k:v for k,v in sorted(evaluation_dict.items(),\\n                                         key=lambda item: item[0],\\n                                         reverse=True)}\\n    return sorted_eval_dict\\n\\nsorted_evals = evaluate_and_sort(best_pipes, X_TestSet, Y_TestSet)\\nprint(len(sorted_evals))\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "'''\n",
        "def evaluate_param_on_test_set(pipe,X_test, Y_test):\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    accuracy = accuracy_score(Y_test, y_pred)\n",
        "    precision = precision_score(Y_test, y_pred)\n",
        "    recall = recall_score(Y_test, y_pred)\n",
        "    f1 = f1_score(Y_test, y_pred)\n",
        "    return ((precision, accuracy, recall, f1), pipe.param_dict)\n",
        "\n",
        "\n",
        "def evaluate_and_sort(fitted_pipes, X_test, Y_test):\n",
        "    evaluations = [evaluate_param_on_test_set(pipe, X_test, Y_test)\n",
        "               for pipe in fitted_pipes]\n",
        "    evaluation_dict = {}\n",
        "    for eval in evaluations:\n",
        "        if eval[0] in evaluation_dict:\n",
        "            evaluation_dict[eval[0]].append(eval[1])\n",
        "        else:\n",
        "            evaluation_dict[eval[0]] = [eval[1]]\n",
        "    sorted_eval_dict = {k:v for k,v in sorted(evaluation_dict.items(),\n",
        "                                         key=lambda item: item[0],\n",
        "                                         reverse=True)}\n",
        "    return sorted_eval_dict\n",
        "\n",
        "sorted_evals = evaluate_and_sort(best_pipes, X_TestSet, Y_TestSet)\n",
        "print(len(sorted_evals))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It turns out that all of these best sets of parameters produce models that perform equally well with respect to the standard meterics. We will have to pick one to deploy. We will look at the time it took to score each model during the grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     mean_score_time  std_score_time  \\\n",
            "81          0.322713        0.144490   \n",
            "108         0.377752        0.131118   \n",
            "135         0.378142        0.145327   \n",
            "65          0.383024        0.131624   \n",
            "184         0.399108        0.108866   \n",
            "\n",
            "                                                params  \n",
            "81   {'corr_selector__threshold': 0.77, 'model__C':...  \n",
            "108  {'corr_selector__threshold': 0.77, 'model__C':...  \n",
            "135  {'corr_selector__threshold': 0.79, 'model__C':...  \n",
            "65   {'corr_selector__threshold': 0.77, 'model__C':...  \n",
            "184  {'corr_selector__threshold': 0.809999999999999...  \n",
            "{'corr_selector__threshold': 0.77, 'model__C': 500.5, 'model__penalty': 'l2', 'model__solver': 'newton-cg'}\n"
          ]
        }
      ],
      "source": [
        "time_results = best_results.filter(['mean_score_time','std_score_time','params'])\n",
        "\n",
        "time_results = time_results.sort_values(by=['mean_score_time','std_score_time'])\n",
        "print(time_results.head())\n",
        "params_choice = time_results.iloc[0]['params']\n",
        "print(params_choice)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have the following choice of hyperparameters:\n",
        "* correlation threshold: 0.77\n",
        "* `C`: 500.5\n",
        "* solver method: newton-cg\n",
        "* penalty function: l2\n",
        "\n",
        "Let's train the model and then look at the classification report. We don't need to list the penalty function since l2 is the default penalty function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### Train Set #### \n",
            "\n",
            "---  Confusion Matrix  ---\n",
            "                Actual loss Actual win\n",
            "Prediction loss       11253       2436\n",
            "Prediction win         2022      19165\n",
            "\n",
            "\n",
            "---  Classification Report  ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        loss       0.85      0.82      0.83     13689\n",
            "         win       0.89      0.90      0.90     21187\n",
            "\n",
            "    accuracy                           0.87     34876\n",
            "   macro avg       0.87      0.86      0.87     34876\n",
            "weighted avg       0.87      0.87      0.87     34876\n",
            " \n",
            "\n",
            "#### Test Set ####\n",
            "\n",
            "---  Confusion Matrix  ---\n",
            "                Actual loss Actual win\n",
            "Prediction loss        2839        652\n",
            "Prediction win          487       4741\n",
            "\n",
            "\n",
            "---  Classification Report  ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        loss       0.85      0.81      0.83      3491\n",
            "         win       0.88      0.91      0.89      5228\n",
            "\n",
            "    accuracy                           0.87      8719\n",
            "   macro avg       0.87      0.86      0.86      8719\n",
            "weighted avg       0.87      0.87      0.87      8719\n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from src.model_eval import clf_performance\n",
        "\n",
        "model_params = {'C':500.5, 'solver': 'newton-cg'}\n",
        "\n",
        "logistic_pipe = create_pipe(model_name='LogisticRegression', params=model_params)\n",
        "logistic_pipe.set_params(corr_selector__threshold=0.77)\n",
        "logistic_pipe.fit(X_TrainSet, Y_TrainSet)\n",
        "\n",
        "clf_performance(X_TrainSet, Y_TrainSet, X_TestSet, Y_TestSet, logistic_pipe, label_map=['loss','win'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance on the training and test data is similar which means that our model is generalizing well. We are hapy with these scores.\n",
        "\n",
        "Let's now see the importance of the different features according to our final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* These are the 8 most important features in descending order. The model was trained on them: \n",
            "['dreb_home', 'dreb_away', 'tov_away', 'tov_home', 'fta_home', 'fta_away', 'ast_home', 'ast_away']\n",
            "fta_home 5.888622613874195\n",
            "dreb_home 13.25997735369944\n",
            "ast_home 5.295534596542505\n",
            "tov_home -6.872948857799825\n",
            "fta_away -5.44340841168103\n",
            "dreb_away -13.186693674418812\n",
            "ast_away -4.631033401411432\n",
            "tov_away 7.690179676996232\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from src.noteboook_functions import feature_importance_logistic_regression\n",
        "\n",
        "\n",
        "feature_importance_logistic_regression(logistic_pipe, 'model', X_TrainSet, TO_DROP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to our logistic model, defensive rebounds and turn overs are the most important features for predicting who will win when point related features are removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats = ['dreb','tov','ast','fta']\n",
        "home_features = [term+'_home' for term in stats]\n",
        "away_features = [term+'_away' for term in stats]\n",
        "features = home_features + away_features\n",
        "to_drop = [feature for feature in X_TrainSet.columns \n",
        "           if feature not in features]\n",
        "\n",
        "TRANSFORMS = {'box_cox':(vt.BoxCoxTransformer,False),\n",
        "              'yeo_johnson':(vt.YeoJohnsonTransformer,False)}\n",
        "TRANSFORM_ASSIGNMENTS = {\n",
        "    'yeo_johnson': ['dreb_away', 'blk_home', 'oreb_away', 'fta_away',\n",
        "                    'dreb_home', 'ast_home', 'stl_away', 'stl_home',\n",
        "                    'reb_away', 'oreb_home', 'pf_away', 'pf_home'],\n",
        "    'box_cox': ['ast_away', 'fta_home']\n",
        "                            }\n",
        "\n",
        "\n",
        "new_assignments = { key: [val for val in value if val not in to_drop] \n",
        "                       for key,value in TRANSFORM_ASSIGNMENTS.items()}\n",
        "   \n",
        "final_logistic_pipe = Pipeline([\n",
        "        ('dropper', DropFeatures(features_to_drop=TO_DROP))])\n",
        "        \n",
        "for transform, targets in new_assignments.items():\n",
        "    if not targets:\n",
        "        continue\n",
        "    final_logistic_pipe.steps.append(\n",
        "            (transform, TRANSFORMS[transform][0](variables=targets))\n",
        "            )\n",
        "final_logistic_pipe.steps.append(('scaler', StandardScaler()))\n",
        "final_logistic_pipe.steps.append(('model',LogisticRegression(C=500.5, \n",
        "                                        solver='newton-cg', random_state=42)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we fit and evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### Train Set #### \n",
            "\n",
            "---  Confusion Matrix  ---\n",
            "                Actual loss Actual win\n",
            "Prediction loss       11401       2288\n",
            "Prediction win         1921      19266\n",
            "\n",
            "\n",
            "---  Classification Report  ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        loss       0.86      0.83      0.84     13689\n",
            "         win       0.89      0.91      0.90     21187\n",
            "\n",
            "    accuracy                           0.88     34876\n",
            "   macro avg       0.87      0.87      0.87     34876\n",
            "weighted avg       0.88      0.88      0.88     34876\n",
            " \n",
            "\n",
            "#### Test Set ####\n",
            "\n",
            "---  Confusion Matrix  ---\n",
            "                Actual loss Actual win\n",
            "Prediction loss        2886        605\n",
            "Prediction win          450       4778\n",
            "\n",
            "\n",
            "---  Classification Report  ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        loss       0.87      0.83      0.85      3491\n",
            "         win       0.89      0.91      0.90      5228\n",
            "\n",
            "    accuracy                           0.88      8719\n",
            "   macro avg       0.88      0.87      0.87      8719\n",
            "weighted avg       0.88      0.88      0.88      8719\n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "final_logistic_pipe.fit(X_TrainSet, Y_TrainSet)\n",
        "clf_performance(X_TrainSet, Y_TrainSet, X_TestSet, Y_TestSet, final_logistic_pipe, label_map=['loss','win'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unsurprisingly, the performance remains quite good. Now on to AdaBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3: AdaBoost\n",
        "We will now do a grid search with AdaBoost. After an initial search to determine a range, we will investigate more closely.\n",
        "\n",
        "With AdaBoost, there are two types of parameters. We have parameters for AdaBoost.\n",
        "\n",
        "AdaBoost has the following hyperparameters.\n",
        "* `n_estimators` the max number of estimators\n",
        "* `learning_rate` which weights the estimators\n",
        "* `algorithm` of which there are two choices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils import divide_range\n",
        "\n",
        "\n",
        "ada_params = [{'corr_selector__threshold': divide_range(0.55,0.95,3),\n",
        "'model__n_estimators':[int(i) for i in divide_range(20,50,3)],\n",
        "'model__learning_rate': divide_range(0.5,2,3),\n",
        "'model__algorithm':['SAMME', 'SAMME.R']\n",
        "}]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_pipe = create_pipe('AdaBoost')\n",
        "ada_results_df_v1 = get_grid_results_df(ada_pipe,'ada_results_df_v1', dir, \n",
        "                                             param_grid=ada_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see what the top scores are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---  Score Counts  ---\n",
            "Precision: 0.8664772614234539, Accuracy: 0.8500976538094769\n",
            "Count: 1\n",
            "\n",
            "Precision: 0.8659743228971937, Accuracy: 0.8490653957449608\n",
            "Count: 1\n",
            "\n",
            "Precision: 0.8656983077474066, Accuracy: 0.8444489904968597\n",
            "Count: 1\n",
            "\n",
            "Precision: 0.8622546188710796, Accuracy: 0.8413808901384368\n",
            "Count: 1\n",
            "\n",
            "---  Score Stats  ---\n",
            "Most Common: Precision: -1\n",
            "             Accuracy: -1\n",
            "             Count: 32\n",
            "Max Score: Precision: 0.8664772614234539\n",
            "           Accuracy: 0.8500976538094769\n",
            "           Count: 1\n",
            "Max Precision: 0.8664772614234539\n",
            "Max Accuracy: 0.8500976538094769\n"
          ]
        }
      ],
      "source": [
        "present_score_counts(ada_results_df_v1)\n",
        "best_score_v1 = score_stats(ada_results_df_v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the parameters associated with the top 5 scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: (0.8664772614234539, 0.8500976538094769)\n",
            "model__n_estimators: 50, Count: 1\n",
            "model__learning_rate: 1.0, Count: 1\n",
            "model__algorithm: SAMME.R, Count: 1\n",
            "corr_selector__threshold: 0.95, Count: 1\n",
            "Score: (0.8659743228971937, 0.8490653957449608)\n",
            "model__n_estimators: 50, Count: 1\n",
            "model__learning_rate: 1.0, Count: 1\n",
            "model__algorithm: SAMME.R, Count: 1\n",
            "corr_selector__threshold: 0.817, Count: 1\n",
            "Score: (0.8656983077474066, 0.8444489904968597)\n",
            "model__n_estimators: 50, Count: 1\n",
            "model__learning_rate: 1.5, Count: 1\n",
            "model__algorithm: SAMME.R, Count: 1\n",
            "corr_selector__threshold: 0.95, Count: 1\n",
            "Score: (0.8622546188710796, 0.8413808901384368)\n",
            "model__n_estimators: 50, Count: 1\n",
            "model__learning_rate: 1.5, Count: 1\n",
            "model__algorithm: SAMME.R, Count: 1\n",
            "corr_selector__threshold: 0.817, Count: 1\n",
            "Score: (0.8620041964216935, 0.8457393665121172)\n",
            "model__n_estimators: 40, Count: 1\n",
            "model__learning_rate: 1.0, Count: 1\n",
            "model__algorithm: SAMME.R, Count: 1\n",
            "corr_selector__threshold: 0.95, Count: 1\n"
          ]
        }
      ],
      "source": [
        "from src.model_eval import collect_like_estimators\n",
        "\n",
        "def top_n(results_df,num=5,exclude=None):\n",
        "    estimators_by_score = collect_like_estimators(results_df)\n",
        "    scores = list(estimators_by_score.keys())\n",
        "    top = sorted(scores,reverse=True)[:num]\n",
        "    for score in top:\n",
        "        print(f\"Score: {score}\")\n",
        "        present_param_counts(results_df, score, exclude)\n",
        "\n",
        "top_n(ada_results_df_v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This gives us an idea of how to narrow down our hyperparameters.\n",
        "* More estimators is better.\n",
        "* We will use the algorithm `'SAMME.R'`.\n",
        "* Learning rate near 1 is good\n",
        "* We will focus on a range of correlation threshold from 0.8 to 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_pipe.set_params(model__algorithm='SAMME.R')\n",
        "ada_params_v2 = [{'corr_selector__threshold': divide_range(0.8,0.95,3),\n",
        "'model__n_estimators':[int(i) for i in divide_range(40,80,3)],\n",
        "'model__learning_rate': divide_range(0.8,1.5,3),}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_results_df_v2 = get_grid_results_df(ada_pipe,'ada_results_df_v2',\n",
        "                                             dir, param_grid=ada_params_v2,\n",
        "                                             verbosity=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameters are for the models with the top scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: (0.878885158961508, 0.8621116618986552)\n",
            "model__n_estimators: 80, Count: 1\n",
            "model__learning_rate: 1.033, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n",
            "Score: (0.877798938119638, 0.8596457244089312)\n",
            "model__n_estimators: 80, Count: 1\n",
            "model__learning_rate: 1.267, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n",
            "Score: (0.8770133758415453, 0.8586708016836013)\n",
            "model__n_estimators: 66, Count: 1\n",
            "model__learning_rate: 1.267, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n",
            "Score: (0.8769633783106618, 0.85941638716254)\n",
            "model__n_estimators: 80, Count: 1\n",
            "model__learning_rate: 1.5, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n",
            "Score: (0.8768317562019001, 0.8636600078918812)\n",
            "model__n_estimators: 80, Count: 1\n",
            "model__learning_rate: 0.8, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n",
            "Score: (0.8767883630046563, 0.8609933782184077)\n",
            "model__n_estimators: 66, Count: 1\n",
            "model__learning_rate: 1.033, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n"
          ]
        }
      ],
      "source": [
        "top_n(ada_results_df_v2,6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems that an increase in the number of estimators improves performance, so we will slide our window up a bit. Unfortunately, this will increase the fit time. We will narrow the focus for the correlation threshold even further. We feel we didn't cast a wide enough net with respect to learning rate, so we will look at a larger window for this parameter as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_params_v3 = [{'corr_selector__threshold': divide_range(0.75,0.85,2),\n",
        "    'model__n_estimators':[int(i) for i in divide_range(70,95,2)],\n",
        "    'model__learning_rate': divide_range(1,1.4,2),}]\n",
        "\n",
        "ada_results_df_v3 = get_grid_results_df(ada_pipe,'ada_results_df_v3',\n",
        "                                             dir, param_grid=ada_params_v3,\n",
        "                                             verbosity=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: (0.8803990370217539, 0.8644915080069712)\n",
            "model__n_estimators: 95, Count: 1\n",
            "model__learning_rate: 1.0, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n",
            "Score: (0.8796066554195157, 0.8615381851961461)\n",
            "model__n_estimators: 95, Count: 1\n",
            "model__learning_rate: 1.4, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n",
            "Score: (0.8792885994909596, 0.8619969213442504)\n",
            "model__n_estimators: 95, Count: 1\n",
            "model__learning_rate: 1.2, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n",
            "Score: (0.8789665965602946, 0.8628858677781066)\n",
            "model__n_estimators: 82, Count: 1\n",
            "model__learning_rate: 1.0, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n",
            "Score: (0.8787931333372656, 0.8612800960178882)\n",
            "model__n_estimators: 82, Count: 1\n",
            "model__learning_rate: 1.2, Count: 1\n",
            "corr_selector__threshold: 0.8, Count: 1\n"
          ]
        }
      ],
      "source": [
        "top_n(ada_results_df_v3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scores did improve, but not significantly. If the improvement is due to values for learning rate and estimators being outside of our earlier ranges, then we will have to continue searching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_pipe.set_params(corr_selector__threshold=0.8)\n",
        "ada_params_v4 = [{'model__n_estimators':[int(i) for i in divide_range(85,100,3)],\n",
        "    'model__learning_rate': divide_range(1,1.4,3),}]\n",
        "\n",
        "ada_results_df_v4 = get_grid_results_df(ada_pipe,'ada_results_df_v4',\n",
        "                                             dir, param_grid=ada_params_v4,\n",
        "                                             verbosity=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: (0.8807316624040382, 0.8649502605964947)\n",
            "model__n_estimators: 100, Count: 1\n",
            "model__learning_rate: 1.0, Count: 1\n",
            "Score: (0.8803990370217539, 0.8644915080069712)\n",
            "model__n_estimators: 95, Count: 1\n",
            "model__learning_rate: 1.0, Count: 1\n",
            "Score: (0.8800809495175151, 0.863545345434218)\n",
            "model__n_estimators: 100, Count: 1\n",
            "model__learning_rate: 1.133, Count: 1\n",
            "Score: (0.8799229144579022, 0.8634879278879353)\n",
            "model__n_estimators: 90, Count: 1\n",
            "model__learning_rate: 1.0, Count: 1\n",
            "Score: (0.8798438080654913, 0.8626850646147777)\n",
            "model__n_estimators: 90, Count: 1\n",
            "model__learning_rate: 1.133, Count: 1\n"
          ]
        }
      ],
      "source": [
        "top_n(ada_results_df_v4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_params_v5 = [{'model__n_estimators':[int(i) for i in divide_range(90,110,3)],\n",
        "    'model__learning_rate': divide_range(1,1.2,3),}]\n",
        "\n",
        "ada_results_df_v5 = get_grid_results_df(ada_pipe,'ada_results_df_v5',\n",
        "                                             dir, param_grid=ada_params_v5,\n",
        "                                             verbosity=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: (0.8817674563402231, 0.8651796882706915)\n",
            "model__n_estimators: 110, Count: 1\n",
            "model__learning_rate: 1.133, Count: 1\n",
            "Score: (0.8813343098897812, 0.8642333941665843)\n",
            "model__n_estimators: 110, Count: 1\n",
            "model__learning_rate: 1.067, Count: 1\n",
            "Score: (0.8812142711278724, 0.864892949919437)\n",
            "model__n_estimators: 110, Count: 1\n",
            "model__learning_rate: 1.0, Count: 1\n",
            "Score: (0.8808039129441457, 0.8641761533655584)\n",
            "model__n_estimators: 103, Count: 1\n",
            "model__learning_rate: 1.133, Count: 1\n",
            "Score: (0.8807565163099091, 0.8638606548617276)\n",
            "model__n_estimators: 103, Count: 1\n",
            "model__learning_rate: 1.067, Count: 1\n"
          ]
        }
      ],
      "source": [
        "top_n(ada_results_df_v5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will take the parameters of the top performer in this last pass.\n",
        "* estimators: 110\n",
        "* learning rate: 1.133\n",
        "* correlation threshold: 0.8\n",
        "* algorithm: SAMME.R\n",
        "\n",
        "Now we need to evaluate our model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### Train Set #### \n",
            "\n",
            "---  Confusion Matrix  ---\n",
            "                Actual loss Actual win\n",
            "Prediction loss       11184       2505\n",
            "Prediction win         2124      19063\n",
            "\n",
            "\n",
            "---  Classification Report  ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        loss       0.84      0.82      0.83     13689\n",
            "         win       0.88      0.90      0.89     21187\n",
            "\n",
            "    accuracy                           0.87     34876\n",
            "   macro avg       0.86      0.86      0.86     34876\n",
            "weighted avg       0.87      0.87      0.87     34876\n",
            " \n",
            "\n",
            "#### Test Set ####\n",
            "\n",
            "---  Confusion Matrix  ---\n",
            "                Actual loss Actual win\n",
            "Prediction loss        2831        660\n",
            "Prediction win          530       4698\n",
            "\n",
            "\n",
            "---  Classification Report  ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        loss       0.84      0.81      0.83      3491\n",
            "         win       0.88      0.90      0.89      5228\n",
            "\n",
            "    accuracy                           0.86      8719\n",
            "   macro avg       0.86      0.85      0.86      8719\n",
            "weighted avg       0.86      0.86      0.86      8719\n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "params={'n_estimators': 110, 'learning_rate': 1.133, 'algorithm': 'SAMME.R'}\n",
        "\n",
        "ada_pipe = create_pipe(model_name='AdaBoost', params=params)\n",
        "ada_pipe.set_params(corr_selector__threshold=0.8)\n",
        "ada_pipe.fit(X_TrainSet, Y_TrainSet)\n",
        "\n",
        "clf_performance(X_TrainSet, Y_TrainSet, X_TestSet, Y_TestSet, ada_pipe, label_map=['loss','win'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance on the training and test sets are very similar. This model is also generalizing well. Now to see about important features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* These are the 8 most important features in descending order. The model was trained on them: \n",
            "['dreb_home', 'dreb_away', 'tov_home', 'tov_away', 'ast_away', 'fta_home', 'ast_home', 'fta_away']\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from src.noteboook_functions import feature_importance_tree_based_models, find_features\n",
        "\n",
        "\n",
        "kept, _ = find_features(X_TrainSet, ada_pipe, TO_DROP)\n",
        "X = X_TrainSet.filter(kept)\n",
        "ada_model = ada_pipe['model']\n",
        "feature_importance_tree_based_models(model=ada_model, columns=X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model also finds that the most important predictor of winning is defensive rebounds and turnovers.\n",
        "\n",
        "Now we will fit our pipeline after dropping all but the above features. This will be our final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### Train Set #### \n",
            "\n",
            "---  Confusion Matrix  ---\n",
            "                Actual loss Actual win\n",
            "Prediction loss       11304       2385\n",
            "Prediction win         1997      19190\n",
            "\n",
            "\n",
            "---  Classification Report  ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        loss       0.85      0.83      0.84     13689\n",
            "         win       0.89      0.91      0.90     21187\n",
            "\n",
            "    accuracy                           0.87     34876\n",
            "   macro avg       0.87      0.87      0.87     34876\n",
            "weighted avg       0.87      0.87      0.87     34876\n",
            " \n",
            "\n",
            "#### Test Set ####\n",
            "\n",
            "---  Confusion Matrix  ---\n",
            "                Actual loss Actual win\n",
            "Prediction loss        2843        648\n",
            "Prediction win          517       4711\n",
            "\n",
            "\n",
            "---  Classification Report  ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        loss       0.85      0.81      0.83      3491\n",
            "         win       0.88      0.90      0.89      5228\n",
            "\n",
            "    accuracy                           0.87      8719\n",
            "   macro avg       0.86      0.86      0.86      8719\n",
            "weighted avg       0.87      0.87      0.87      8719\n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "stats = ['dreb','tov','ast','fta']\n",
        "home_features = [term+'_home' for term in stats]\n",
        "away_features = [term+'_away' for term in stats]\n",
        "features = home_features + away_features\n",
        "to_drop = [feature for feature in X_TrainSet.columns \n",
        "           if feature not in features]\n",
        "\n",
        "TRANSFORMS = {'box_cox':(vt.BoxCoxTransformer,False),\n",
        "              'yeo_johnson':(vt.YeoJohnsonTransformer,False)}\n",
        "TRANSFORM_ASSIGNMENTS = {\n",
        "    'yeo_johnson': ['dreb_away', 'blk_home', 'oreb_away', 'fta_away',\n",
        "                    'dreb_home', 'ast_home', 'stl_away', 'stl_home',\n",
        "                    'reb_away', 'oreb_home', 'pf_away', 'pf_home'],\n",
        "    'box_cox': ['ast_away', 'fta_home']\n",
        "                            }\n",
        "\n",
        "\n",
        "new_assignments = { key: [val for val in value if val not in to_drop] \n",
        "                       for key,value in TRANSFORM_ASSIGNMENTS.items()}\n",
        "   \n",
        "final_ada_pipe = Pipeline([\n",
        "        ('dropper', DropFeatures(features_to_drop=TO_DROP))])\n",
        "        \n",
        "for transform, targets in new_assignments.items():\n",
        "    if not targets:\n",
        "        continue\n",
        "    final_logistic_pipe.steps.append(\n",
        "            (transform, TRANSFORMS[transform][0](variables=targets))\n",
        "            )\n",
        "final_ada_pipe.steps.append(('scaler', StandardScaler()))\n",
        "final_ada_pipe.steps.append(('model',AdaBoostClassifier(n_estimators=110, \n",
        "                                    learning_rate=1.133, algorithm='SAMME.R', \n",
        "                                    random_state=42)))\n",
        "\n",
        "\n",
        "final_ada_pipe.fit(X_TrainSet, Y_TrainSet)\n",
        "clf_performance(X_TrainSet, Y_TrainSet, X_TestSet, Y_TestSet, final_ada_pipe, label_map=['loss','win'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our Logistic Regression performs a bit better. But both models meet our criteria of 75% average precision and 70% accuracy.\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "We have constructed two models that exceed our business requirements. On unseen data, they score:\n",
        "* Logistic Regression:\n",
        "* * Avg. Precision:\n",
        "* * Accuracy:\n",
        "* Adaptive Boost:\n",
        "* * Avg. Precision: 86.5%\n",
        "* * Accuracy: 87%\n",
        "\n",
        "Let's finish by saving our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['outputs/ml_pipeline/predict_home_wins/v1/ada_pipeline.pkl']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# taken from Churnometer walkthrough project\n",
        "import joblib\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/predict_home_wins/{version}'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "joblib.dump(value=final_logistic_pipe, filename=f\"{file_path}/logistic_pipeline.pkl\")\n",
        "joblib.dump(value=final_ada_pipe, filename=f\"{file_path}/ada_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Next Steps\n",
        "In the next notebook we will do some clustering analysis of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
