{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "* Do a grid search using cross-validation in order to select a classification model. We will run this grid search several times removing different features in order to test our Hytpothesis # something and construct non-trivial models.\n",
        "\n",
        "## Inputs\n",
        "* The train and test data set aside in the the last notebook.\n",
        "* The pipeline that was produced in the last notebook.\n",
        "* Parameter values determined in previous notebook.\n",
        "\n",
        "## Outputs\n",
        "* A choice of classification model that we will further tune and evaluate.\n",
        "\n",
        "## Additional Comments\n",
        "* We have chosen to do classification. We may yet do a regression model on the point differential.      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory\n",
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/pp5-ml-dashboard\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "home_dir = '/workspace/pp5-ml-dashboard'\n",
        "os.chdir(home_dir)\n",
        "current_dir = os.getcwd()\n",
        "print(current_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now load our training and test sets, as well as some of the packages that we will be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from src.utils import get_df, save_df\n",
        "\n",
        "train_dir = 'train/csv'\n",
        "X_TrainSet = get_df('X_TrainSet',train_dir)\n",
        "Y_TrainSet = get_df('Y_TrainSet',train_dir)\n",
        "\n",
        "test_dir = 'test/csv'\n",
        "X_TestSet = get_df('X_TestSet',test_dir)\n",
        "Y_TestSet = get_df('Y_TestSet',test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Full Pipeline\n",
        "We will build the full pipeline here. It will accept some parameters so that we can tune it later. We also declare some constants that we established in the last notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.selection import DropFeatures, SmartCorrelatedSelection\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Constants needed for feature engineering\n",
        "META = ['season', 'play_off']\n",
        "TRIVIAL = ['pts_home','pts_away','plus_minus_home']\n",
        "THRESH = 0.85\n",
        "TRANSFORMS = {'log_e':(vt.LogTransformer, False), \n",
        "                'log_10':(vt.LogTransformer,'10'),\n",
        "                'reciprocal':(vt.ReciprocalTransformer,False), \n",
        "                'power':(vt.PowerTransformer,False),\n",
        "                'box_cox':(vt.BoxCoxTransformer,False),\n",
        "                'yeo_johnson':(vt.YeoJohnsonTransformer,False)}\n",
        "TRANSFORM_ASSIGNMENTS = {\n",
        "    'yeo_johnson': ['dreb_away', 'blk_home', 'oreb_away', 'fta_away', 'dreb_home', \n",
        "                    'ast_home', 'stl_away', 'pts_away', 'stl_home', 'reb_away',\n",
        "                    'pts_home', 'fgm_away', 'oreb_home', 'pf_away', 'pf_home'],\n",
        "    'box_cox': ['ast_away', 'fta_home']\n",
        "                            }\n",
        "\n",
        "\n",
        "def base_pipeline(to_drop=None,thresh=THRESH):\n",
        "    if not to_drop:\n",
        "        to_drop = META\n",
        "    else:\n",
        "        to_drop.extend(META)\n",
        "    to_drop = list(set(to_drop))\n",
        "    pipeline = Pipeline([\n",
        "        ('dropper', DropFeatures(features_to_drop=to_drop)),\n",
        "        ('corr_selector', SmartCorrelatedSelection(method=\"pearson\",\n",
        "                                                   threshold=thresh,\n",
        "                                                   selection_method=\"variance\")\n",
        "                                                   )\n",
        "                        ])\n",
        "    pipeline.to_drop = to_drop\n",
        "    return pipeline\n",
        "\n",
        "    \n",
        "def add_transformations(pipeline, transform_assignments):\n",
        "    # This needs to be called after the above is fit so that the correlation selector has that attr\n",
        "    to_drop = pipeline.to_drop\n",
        "    dropping = set(to_drop + pipeline['corr_selector'].features_to_drop_)\n",
        "    \n",
        "    new_assignments = { key: [val for val in value if val not in dropping] \n",
        "                       for key,value in transform_assignments.items()}\n",
        "    ''' Olde version\n",
        "    for value in transform_assignments.values():\n",
        "        for drop_term in dropping:\n",
        "            if drop_term in value:\n",
        "                value.remove(drop_term)\n",
        "    '''\n",
        "    for transform, targets in new_assignments.items():\n",
        "        if not targets:\n",
        "            continue\n",
        "        pipeline.steps.append(\n",
        "            (transform, TRANSFORMS[transform][0](variables=targets))\n",
        "            )\n",
        "    pipeline.steps.append(('scaler', StandardScaler()))\n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add the last two steps of the pipeline, the feature selection and the model itself. We do this in a separate step since it depends on the model.\n",
        "\n",
        "After a first pass, we will see that our models only use the features in `TRIVIAL`. These features make the classification problem too ... trivial as the winner of the game can be computed directly from `'plus_minus_home'`. The points of the each team can be used together to compute the winner as well, so we also remove them. \n",
        "\n",
        "# Attention\n",
        "this could probably be implimented better with classes, but I think I don't want to worry too much about inheritance, I could do it with composition, and then tune the hyperparameters via methods. Do that later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "MODELS = {\n",
        "    'LogisticRegression': LogisticRegression,\n",
        "    'DecisionTree': DecisionTreeClassifier,\n",
        "    'RandomForest': RandomForestClassifier,\n",
        "    'GradientBoosting': GradientBoostingClassifier,\n",
        "    'ExtraTrees': ExtraTreesClassifier,\n",
        "    'AdaBoost': AdaBoostClassifier,\n",
        "    'XGBoost': XGBClassifier\n",
        "}\n",
        "\n",
        "\n",
        "def add_feat_selection_n_model(pipeline,model_name,random_state=42):\n",
        "    model = MODELS[model_name](random_state=random_state)\n",
        "    pipeline.steps.append((\"feat_selection\", SelectFromModel(model)))\n",
        "    pipeline.steps.append((model_name,model))\n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now create our list of models that we are going to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_pipelines(to_drop=None,thresh=THRESH):\n",
        "    PIPELINES = {}\n",
        "    for model_name in MODELS:\n",
        "        base_pipe = base_pipeline(to_drop,thresh)\n",
        "        base_pipe.fit(X_TrainSet)\n",
        "        pipe_w_transforms= add_transformations(base_pipe,TRANSFORM_ASSIGNMENTS)\n",
        "        PIPELINES[model_name] = add_feat_selection_n_model(pipe_w_transforms,model_name)\n",
        "    return PIPELINES\n",
        "PIPELINES = create_pipelines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Section 2: Cross Validation Search\n",
        "We are going to perform multiple grid searchs as we refine our selection of features. The final grid search will determine the best model for our data, which we will tune in the next notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention below cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('dropper',\n",
              "                 DropFeatures(features_to_drop=['play_off', 'season'])),\n",
              "                ('corr_selector',\n",
              "                 SmartCorrelatedSelection(selection_method='variance',\n",
              "                                          threshold=0.85)),\n",
              "                ('feature_selection',\n",
              "                 SelectFromModel(estimator=RandomForestClassifier(random_state=42))),\n",
              "                ('model', RandomForestClassifier(random_state=42))])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# this cell was for dev and can be deleted later\n",
        "pipe1 = PIPELINES['LogisticRegression']\n",
        "pipe1\n",
        "rand_forest_pipe = base_pipeline()\n",
        "rand_forest_pipe.steps.append(('feature_selection', SelectFromModel(RandomForestClassifier(random_state=42))))\n",
        "rand_forest_pipe.steps.append(('model',RandomForestClassifier(random_state=42)))\n",
        "#logistic_pipe = add_feat_selection_n_model(logistic_pipe,'LogisticRegression')\n",
        "rand_forest_pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a few different Hypotheses. One of them, hypothesis 1, is that the models will gravitate strongly towards the features related to points. In order of estimated impact, they are:\n",
        "* `'plus_minus'`\n",
        "* `'pts'`\n",
        "* `'fgm'`\n",
        "* `'fg3m'`, `'ftm'`\n",
        "\n",
        "We will start by training our model on all of these features. To test our hypothesis, we will remove one collection of features at a time until we are left with no features that help you directly determine the score (and hence winner). We will check which features the models use along the way to see if we can validate our hypothesis. We expect that the earlier models will be able to predict the outcome of the games flawlessly.\n",
        "\n",
        "If you wish to get immediate information, set the verbosity to 3. It will report on how the model is performing during each step of the grid search. Or you could wait and see the performance in the following cell.\n",
        "\n",
        "Note: we are silencing warnings below. These warnings generate  hundreds of lines and no exceptions are raised."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "param_grid = {\"model__n_estimators\":[10*i for i in range(1,10)],}\n",
        "\n",
        "def grid_search(X_train, y_train, pipelines,param_grid={}):\n",
        "    GRIDS = {}\n",
        "    for pipe_name, pipe in pipelines.items():\n",
        "        print(f\"### Beginning grid search for {pipe_name} ###\")\n",
        "        try:\n",
        "            grid=GridSearchCV(estimator=pipe,\n",
        "                    param_grid=param_grid,\n",
        "                    cv=5,\n",
        "                    n_jobs=-2,\n",
        "                    verbose=3,\n",
        "                    scoring='accuracy')\n",
        "            grid.fit(X_train,y_train)\n",
        "        except ValueError as e:\n",
        "            msg = \"Invalid parameter model for estimator\"\n",
        "            if msg not in str(e):\n",
        "                raise e\n",
        "            print(\"An estimator was passed invalid parameters. We will \"\n",
        "                  f\"continue the grid search for {pipe_name} with \"\n",
        "                  \"param_grid=\\{\\}\")\n",
        "            print(f\"### Beginning grid search for {pipe_name} with empty params. ###\")\n",
        "            grid=GridSearchCV(estimator=pipe,\n",
        "                    param_grid={},\n",
        "                    cv=5,\n",
        "                    n_jobs=-2,\n",
        "                    verbose=3,\n",
        "                    scoring='accuracy')\n",
        "            grid.fit(X_train,y_train)\n",
        "        GRIDS[pipe_name] = grid\n",
        "    return GRIDS\n",
        "GRIDS = grid_search(X_TrainSet,Y_TrainSet,PIPELINES, param_grid)\n",
        "print(len(GRIDS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the following code to generate reports for evaluating our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Beginning grid search for LogisticRegression ###\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "An estimator was passed invalid parameters. We will continue the grid search for LogisticRegression with param_grid={}\n",
            "### Beginning grid search for LogisticRegression with empty params. ###\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/selection/smart_correlation_selection.py:271: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  f = X[feature_group].std().sort_values(ascending=False).index[0]\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END ..................................., score=1.000 total time=   1.5s\n",
            "[CV 2/5] END ..................................., score=1.000 total time=   1.5s\n",
            "[CV 4/5] END ..................................., score=1.000 total time=   1.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n",
            "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END ..................................., score=1.000 total time=   1.4s\n",
            "[CV 3/5] END ..................................., score=1.000 total time=   1.4s\n"
          ]
        }
      ],
      "source": [
        "# warning check\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import sys\n",
        "\n",
        "param_grid = {\"model__n_estimators\":[10*i for i in range(1,3)],}\n",
        "pipe_name = list(PIPELINES.keys())[0]\n",
        "pipe = PIPELINES[pipe_name]\n",
        "print(f\"### Beginning grid search for {pipe_name} ###\")\n",
        "sys.stderr = open(os.devnull, \"w\")  # silence stderr\n",
        "try:\n",
        "    grid=GridSearchCV(estimator=pipe,\n",
        "                    param_grid=param_grid,\n",
        "                    cv=5,\n",
        "                    n_jobs=-2,\n",
        "                    verbose=3,\n",
        "                    scoring='accuracy')\n",
        "    \n",
        "\n",
        "    grid.fit(X_TrainSet,Y_TrainSet)   \n",
        "except ValueError as e:\n",
        "    msg = \"Invalid parameter model for estimator\"\n",
        "    if msg not in str(e):\n",
        "        raise e\n",
        "    print(\"An estimator was passed invalid parameters. We will \"\n",
        "                  f\"continue the grid search for {pipe_name} with \"\n",
        "                  \"param_grid={}\")\n",
        "    print(f\"### Beginning grid search for {pipe_name} with empty params. ###\")\n",
        "    grid=GridSearchCV(estimator=pipe,\n",
        "                    param_grid={},\n",
        "                    cv=5,\n",
        "                    n_jobs=-2,\n",
        "                    verbose=3,\n",
        "                    scoring='accuracy')\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        grid.fit(X_TrainSet,Y_TrainSet)\n",
        "sys.stderr = sys.__stderr__ \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.model_eval import confusion_matrix_and_report, get_best_scores\n",
        "\n",
        "\n",
        "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
        "  print(\"#### Train Set #### \\n\")\n",
        "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
        "  print(\"#### Test Set ####\\n\")\n",
        "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
        "\n",
        "get_best_scores(GRIDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As there are several different "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, grid in GRIDS.items():\n",
        "    best_pipe = grid.best_estimator_\n",
        "    print(name)\n",
        "    clf_performance(X_TrainSet,Y_TrainSet,X_TestSet,Y_TestSet,best_pipe, label_map=['loss', 'win'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "We are not surprised to see the high level of accuracy. The points scored in the game literally determine who wins, and we have a lot of that data still present. Let's recall which features we train on. Some features that we dropped were common to each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(META)\n",
        "pipe1 = PIPELINES['LogisticRegression']\n",
        "pipe1[:2].fit(X_TrainSet,Y_TrainSet)\n",
        "pipe1['corr_selector'].__dir__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the end, each model was only trained an the following features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_auto_dropped = META + list(pipe1['corr_selector'].features_to_drop_)\n",
        "remaining_cols = [col for col in X_TrainSet.columns if col not in cols_auto_dropped]\n",
        "def find_features(X,Y,pipe):\n",
        "    pipe.fit(X,Y)\n",
        "    features = pipe['feat_selection'].get_support()\n",
        "    X = X_TrainSet.filter(remaining_cols)\n",
        "    if len(X.columns) != features.shape[0]:\n",
        "        raise ValueError\n",
        "    for index, col in enumerate(X.columns):\n",
        "        if not features[index]:\n",
        "            X.drop(col, axis=1, inplace=True)\n",
        "    return X.columns\n",
        "\n",
        "\n",
        "for pipe_name, pipe in NEW_PIPELINES.items():\n",
        "    print(pipe_name)\n",
        "    print(find_features(X_TrainSet,Y_TrainSet,pipe))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how the training goes if we remove `'plus_minus_home'`, `'pts_home'`, and `'pts_away'`. We expect the model to be able to predict the winner solely based on these. Since we will be dropping them before we select features using correlation or `SelectFromModel`, we expect this to have an impact on the models. If you only remove `'plus_minus_home'`, this impacts the performance, but barely. The models fail to be perfect, however, the worst scoring models, ExtraTrees and AdaBoost, have a lowest score of 97%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NEW_PIPELINES = create_pipelines()\n",
        "'''fitted_pipelines = {}\n",
        "for pipe_name, pipe in NEW_PIPELINES.items():\n",
        "    pipe.fit(X_TrainSet,Y_TrainSet)\n",
        "    fitted_pipelines[pipe_name] = pipe\n",
        "'''\n",
        "GRIDS_wo_pm = grid_search(X_TrainSet,Y_TrainSet,NEW_PIPELINES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_best_scores(GRIDS):\n",
        "  for grid in GRIDS.values():\n",
        "    res = (pd.DataFrame(grid.cv_results_)\n",
        "       .sort_values(by='mean_test_score',ascending=False)\n",
        "       .filter(['params','mean_test_score'])\n",
        "       .values)\n",
        "\n",
        "    print(res)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
        "\n",
        "  prediction = pipeline.predict(X)\n",
        "\n",
        "  print('---  Confusion Matrix  ---')\n",
        "  print(pd.DataFrame(confusion_matrix(y_pred=prediction, y_true=y),\n",
        "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
        "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
        "        ))\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "  print('---  Classification Report  ---')\n",
        "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
        "\n",
        "\n",
        "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
        "  print(\"#### Train Set #### \\n\")\n",
        "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
        "\n",
        "  print(\"#### Test Set ####\\n\")\n",
        "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After examining the reports for all of the models in `GRIDS` and those in our new search `GRIDS_wo_pm`, we found that the two worst performing models are still very close to being 100% accurate. They are AdaBoost and ExtraTrees. Other models did fail to be perfect, but these were the only two that did not score 100% across the board on all metrics for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, grid in GRIDS_wo_pm.items():\n",
        "    print(name)\n",
        "    best_estimator = grid.best_estimator_\n",
        "    clf_performance(X_TrainSet,Y_TrainSet,X_TestSet,Y_TestSet,best_estimator, label_map=['loss', 'win']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_ada = GRIDS_wo_pm['AdaBoost'].best_estimator_\n",
        "best_extra_tree = GRIDS_wo_pm['ExtraTrees'].best_estimator_\n",
        "print(\"Ada Boost\")\n",
        "clf_performance(X_TrainSet,Y_TrainSet,X_TestSet,Y_TestSet,best_ada, label_map=['loss', 'win'])\n",
        "\n",
        "print(\"Extra Tree\")\n",
        "clf_performance(X_TrainSet,Y_TrainSet,X_TestSet,Y_TestSet, best_extra_tree, label_map=['loss', 'win'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's inspect which features our models used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for pipe_name, pipe in NEW_PIPELINES.items():\n",
        "    print(pipe_name)\n",
        "    print(find_features(X_TrainSet,Y_TrainSet,pipe))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
