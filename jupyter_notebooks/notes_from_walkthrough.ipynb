{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(ADD HERE THE NOTEBOOK NAME)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Write here your notebook objective, for example, \"Fetch data from Kaggle and save as raw data\", or \"engineer features for modelling\"\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Write here which data or information you need to run the notebook \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "My plan for this notebook is to take notes that I can refer to about different steps, I guess I already did this for the data collection notebook in the playground a bit.\n",
        "\n",
        "So I will take notes on the different steps and maybe copy some python commands but I won't execute anything.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 Data study\n",
        "\n",
        "this is the data understanding step\n",
        "\n",
        "They use ydata_profiling to make a ProfileReport\n",
        "```\n",
        "from ydata_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=df, minimal=True)\n",
        "pandas_report.to_notebook_iframe()\n",
        "```\n",
        "\n",
        "They one hot encode the categorical variables then do a correlation study with both pearson and spearman. They pick the top 5 correlated variables. They sort by absolute value of the correlation.\n",
        "\n",
        "Then they create a df that just uses these features and the target\n",
        "\n",
        "```\n",
        "df_eda = df.filter(vars_to_study + ['Churn'])\n",
        "df_eda.head(3)\n",
        "```\n",
        "\n",
        "They write some code to loop through and plot different things depending on the type of data, categorical vs numerical. vars to study is the list of top 5 correlated features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'vars_to_study' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     22\u001b[0m target_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChurn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvars_to_study\u001b[49m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df_eda[col]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     25\u001b[0m         plot_categorical(df_eda, col, target_var)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vars_to_study' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "\n",
        "def plot_categorical(df, col, target_var):\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    sns.countplot(data=df, x=col, hue=target_var, order=df[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.title(f\"{col}\", fontsize=20, y=1.05)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_numerical(df, col, target_var):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.histplot(data=df, x=col, hue=target_var, kde=True, element=\"step\")\n",
        "    plt.title(f\"{col}\", fontsize=20, y=1.05)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "target_var = 'Churn'\n",
        "for col in vars_to_study:\n",
        "    if df_eda[col].dtype == 'object':\n",
        "        plot_categorical(df_eda, col, target_var)\n",
        "        print(\"\\n\\n\")\n",
        "    else:\n",
        "        plot_numerical(df_eda, col, target_var)\n",
        "        print(\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They do some feature engineering. They put one of the statistics into bins. I am not sure if this is relevant for me. They did this because most of the features they were looking at were categorical and not numerical. Since most of mine, all?, are numerical I don't think this will be relevant at all.\n",
        "They then did a parallel plot, I forget what the analogue of this is for numerical data, maybe a pair plot?\n",
        "\n",
        "This then seems to be sufficient analysis for answering the first business concern: which statistics are predictors of things. I think I was going to use predictive power and correlation. So that would be done first to isolate the most predictive features and then you plot the most predictive features against the target to see what the profile is of the target in terms of these features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now they are doing cleaning and imputation.\n",
        "\n",
        "They use the following link https://docs.google.com/spreadsheets/d/1Pa04ng4d8tjBizKsUCUiUTH-eUNQunSN/edit?gid=1633765441#gid=1633765441\n",
        "\n",
        "So I guess I should make a spreadsheet with rows for the features and columns like missing data, data type, percentage of data (not sure what this one means), and then description of how I am going to clean wrpt that feature.\n",
        "\n",
        "I guess it woouldn't need to be a google spreadsheet, it could be something I include in the readme, or in a notebook.\n",
        "\n",
        "So they plot the distribution of the features with missing data. I think I will probably drop the missing data, since I think it is from older games and can't really be properly imputed since levels of various statistics changed over the years (like )\n",
        "```\n",
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data\n",
        "```\n",
        "They then do a report on all the vars with missing data using the ydata thing.\n",
        "\n",
        "Then they have this custom code for displaying correlation and pps\n",
        "```\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
        "                    linewidth=0.5\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\")\n",
        "    df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"It evaluates monotonic relationship \\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)\n",
        "```\n",
        "\n",
        "There is very little cleaning. I don't think I will have much cleaning either.\n",
        "\n",
        "maybe there will be certain features that I drop because they are the target.\n",
        "\n",
        "They do the train test split before they clean the data.\n",
        "\n",
        "since they remove cols they don't reassess the distribution, I will be removing rows, so I should reassess the distributions.\n",
        "\n",
        "double check to see if there is more missing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 4?\n",
        "\n",
        "feature engineering notebook\n",
        "\n",
        "they also have a spreadsheet for different potential transformations regarding each feature that is still there. They encode the categorical data, I guess there is some of that for me like the team\n",
        "\n",
        "there are some custom functions, I pasted them into utils\n",
        "\n",
        "smart correlation (feature selection thing) should always be last.\n",
        "\n",
        "we want to try and transform the numerical data so that it looks normally distributed.\n",
        "\n",
        "After you do the transformations you need to evaluate how they did. For categorical, this is sort of pointless, since it just works. For the transformations like rescaling with log etc, this is relevant, you want to pick the one that is closest to a normal distribution shape. So I guess that is why we have that step.\n",
        "\n",
        "Then we create the relevant encoders and apply them to the training set and the test set.\n",
        "\n",
        "In the walkthrough, they didn't apply any numerical transformations.\n",
        "\n",
        "then comes smart correlation. we don't need to list which variables we will be dealing with here.\n",
        "\n",
        "This then gives us the steps to add to the pipeline., or the first half of them I guess.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 5\n",
        "modelling and pipeline etc.\n",
        "\n",
        "they load the data and drop the some columns. These are columns that are the targets of the different algorithms, as well as the features that were deemed not suitable for other reasons (this is where I would remove things like name or whatever.)\n",
        "\n",
        "Then they build the pipeline. first encoding categorical data. They would transform the numerical data to be more of a normal distribution if they had found a transformation that did this. Similar to outliers. Then doing the correlation sampler thing where redundant features are removed.\n",
        "\n",
        "They then get to other parts of the pipeline. They didn't just do the smart correlation selection, they are also going to do the feature selection from scikit learn.\n",
        "\n",
        "They then did scaling and select from model.\n",
        "They loaded a bunch of different algorithms from sklearn and are going to test each of them.\n",
        "\n",
        "Remember that if I use a neural network I won't need to remove correlated things etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "\n",
        "            model = PipelineClf(self.models[key])\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "        return df[columns], self.grid_searches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above is stuff for doing hyper parameter selection. I guess I can use it. I can also tinker with it.\n",
        "\n",
        "Then you do the test train split.\n",
        "\n",
        "I guess there are sort of two pipelines, a cleaning and data engineering one, and we fit that pipeline to X_train, then transfrom both X_train and X_test (for X_train, we pass it to the fit_transform method of the pipeline.)\n",
        "\n",
        "There is also the classification pipeline, which has the scalar and feature selection in it. This is weird, maybe ask mo about it?\n",
        "\n",
        "I won't be doing any stuff with imbalanced datasets I don't think.\n",
        "\n",
        "Then the gridsearch cross validation begins. So you have to make a bunch of pipelines by passing models to them and then also making dictionaries for the parameters, these were all empty...? I think this is because we are doing the hyper parameters later.\n",
        "\n",
        "yeah, they do the hyper parameters after they have picked which model they are going to do, I don't know if I will do it that way.\n",
        "\n",
        "There is a thing that happens where several choices of hyper parameters seem to perform the same, what are we supposed to do then?\n",
        "\n",
        "Remember to check the performance against the metrics set out at the beginning. We also want the performance on the training and test set to be similar. if it is much better on the training set then that is an overfit.\n",
        "\n",
        "They then go back and rebuild the pipeline? They do this so that in production the pipeline only has the most important features. I am not sure how I will address this.\n",
        "\n",
        "then retrain the model on the best features with the desired parameters. Since we are only using the best features they drop the feature sleection steps of the pipeline.\n",
        "\n",
        "Then they save the data appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section I forget which number 7?\n",
        "\n",
        "Now they move on to predicting tenure. So this is a regression problem.\n",
        "\n",
        "Again, they use ordinal encoder, smart correlation selection, standard scaler, select from model, used some tree based regression models, and a linear regression.\n",
        "\n",
        "they only care about predicting tenure for users that will churn, what is the analogue of this with my problem?\n",
        "\n",
        "here they only have one pipeline, why? they say it is because they have to address target imbalance.\n",
        "\n",
        "Then they get the best features with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# after data cleaning and feature engineering, the features may have changes\n",
        "# how many data cleaning and feature engineering steps does your pipeline have?\n",
        "data_cleaning_feat_eng_steps = 2\n",
        "columns_after_data_cleaning_feat_eng = (Pipeline(best_regressor_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[best_regressor_pipeline['feat_selection'].get_support(\n",
        ")].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': columns_after_data_cleaning_feat_eng[best_regressor_pipeline['feat_selection'].get_support()],\n",
        "    'Importance': best_regressor_pipeline['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the performance was so poor, they say we should try doing PCA instead of feature selection.\n",
        "\n",
        "They transform the data (scale it etc) and then pass it to some pca thing.\n",
        "\n",
        "They get at least 80% of the variance with 7 components, and then they rebuild the pipeline using 7 components for pca (this explains 72% of the variance, and is just an initial trial, I guess), and they put that in place of the select from model feature selection step. they still use the smart selection correlation thing. Then they redo the grid search to determine the best model and then best hyper parameters.\n",
        "\n",
        "We still are not meeting the requirements from the business case step. So ideally, we would spend more time tuning the hyperparameters. When this still doesn't work, then we can convert the ml task from regression to classification by using bins.\n",
        "\n",
        "since we change from regression to classification, we have to talk to stakeholders about new thresholds for success. because of the business case, they wanted to know users that will churn in the short term so we care about recall on the 4month or less class. The model did meet our requirements, but shit the bed on others.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 8 Cluster Analysis\n",
        "They make a pipeline for doing cluster analysis.\n",
        "\n",
        "In their pipeline, they have the ordinal encoder, smart correlation selection, scaler, PCA, and then a Kmeans unsupervised model. They truncate the pipeline up to PCA (and including) to figure out how many components they should take. You always start with the same number of components as there are features, at least. Then they update the hyperparameter in the PCA step.\n",
        "\n",
        "Also, they start with the n_clusters=50 as the hyperparam for kmeans model. Then Look at the elbow and do silhouette scores. Pick the number of clusters with a good (highest avg maybe?) silhouette score. The we fit the pipeline and add a col to the df for cluster. We then decide to try and fit a classifier to predict the cluster. We are reverse engineering what the profiles of the clusters are by doing the classification, and then looking at the important features for that classification pipeline.\n",
        "\n",
        "lots of custom functions here for displaying stuff. I guess I will just go back to the notebook to get the code if I need/want it.\n",
        "\n",
        "once you have the important features for classifying the clusters, then you redo the clustering model where you only have those features. You need redo all of the analysis and see if it is about the same as the previous clustering model with all of the features. We want to reduce the number of features for performance (like computationally?) reasons, but we don't want to lose quality. Then we have to do the classification model again to see if the clusters we get form the new algorithm have the same profile (not identical, but comparable) and the same important features.\n",
        "\n",
        "In the example, the clustering with fewer features was improved in fact.\n",
        "\n",
        "Then save the figures and the models etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 9?\n",
        "Developing the dashboard.\n",
        "\n",
        "Come up with a plan for the dashboard, like how many pages and what goes on each page.\n",
        "\n",
        "- Intro page\n",
        "    - Should explain project goals like hypothesis and business case\n",
        "    - explain special terms for data\n",
        "    - give opportunity to look at data\n",
        "    - give reference to data\n",
        "    - summary from a notebook showing what we found in the correlation study, I guess?\n",
        "    - then there should be some graphs, not sure what kind to have. they had a parallel plot and churn distributed with respect to the different correlated variables.\n",
        "- page to make predictions\n",
        "    - not sure how I should do this. I could just have them select games from the most recent season which I do not train my data on.\n",
        "- project hypothesis and validation page, just text explaining things\n",
        "- pages with reports\n",
        "    - one for each pipeline\n",
        "    - blurb explaining the conclusions related to that pipeline\n",
        "    - report with metrics related to that pipeline, like classification report stuff and maybe some graphs\n",
        "- pages have a similar structure, code wise\n",
        "    - load data from where it was saved, like df, model, and figures\n",
        "    - write some general conclusioins we learned form the model\n",
        "    - present some graphics and or data that support this\n",
        "    - also explain what the pipeline is in each case\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
